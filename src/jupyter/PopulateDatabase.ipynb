{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436ca00e-6bc0-4bb6-9134-ece22fb241c8",
   "metadata": {},
   "source": [
    "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0.\n",
    "\n",
    "If a copy of the MPL was not distributed with this file, You can obtain one at https://mozilla.org/MPL/2.0/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bf962-019c-40a8-9996-9cf366fd7c50",
   "metadata": {},
   "source": [
    "# Populating the database\n",
    "\n",
    "This notebook will guide you through the process of adding data to the database.\n",
    "\n",
    "First we import the required libraries and check the connection works.\n",
    "\n",
    "**Note for CGP**:\n",
    "Because containers are spun up only when \"poked\", you may need to run this twice in order to give the container time to spin up if you receive a `TimeoutError` after making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92716416-e32a-4944-9af2-23b9ac53d576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:21:12.301164Z",
     "iopub.status.busy": "2023-01-09T16:21:12.300877Z",
     "iopub.status.idle": "2023-01-09T16:21:12.867688Z",
     "shell.execute_reply": "2023-01-09T16:21:12.866960Z",
     "shell.execute_reply.started": "2023-01-09T16:21:12.301118Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to flowkit-ui-backend!\n",
      "{\n",
      "    \"datetime\": \"2023-01-09T16:21:12.829954+00:00\",\n",
      "    \"docker_image\": \"flowminder/flowkit-ui-backend:448133d\",\n",
      "    \"git_branch\": \"main\",\n",
      "    \"git_commit\": \"448133d\",\n",
      "    \"git_tag\": null,\n",
      "    \"python_package\": \"flowkit-ui-backend\",\n",
      "    \"python_version\": \"3.9.16\",\n",
      "    \"api_version\": \"1.2.1\",\n",
      "    \"api_version_url_appendix\": \"v1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import asyncio\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "AUDIENCE = \"https://flowkit-ui-backend.flowminder.org\"\n",
    "BASE_URL = \"http://flowkit-ui-backend:5000/v1\"\n",
    "AUTH0_DOMAIN = \"\"\n",
    "AUTH0_CLIENT_ID_UPDATER = \"\"\n",
    "AUTH0_CLIENT_SECRET_UPDATER = \"\"\n",
    "AUTH0_CLIENT_ID_ADMIN = \"\"\n",
    "AUTH0_CLIENT_SECRET_ADMIN = \"\"\n",
    "\n",
    "\n",
    "def log(response):\n",
    "    to_print = f\"{response.status_code}: \" if response.status_code != 200 else \"\"\n",
    "    if hasattr(response, \"content\") and response.content is not None and response.content != b\"\":\n",
    "        try:\n",
    "            to_print += json.dumps(json.loads(response.content), indent=4)\n",
    "        except Exception as e:\n",
    "            to_print += \"<could not decode response>\"\n",
    "    else:\n",
    "        to_print += \"<no content>\"\n",
    "    print(to_print)\n",
    "\n",
    "\n",
    "response = httpx.get(f\"{BASE_URL}/heartbeat\")\n",
    "\n",
    "print(f\"Welcome to {os.getenv('APP_NAME')}!\")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf8c47-3647-429f-9a0c-38968c4c0b72",
   "metadata": {},
   "source": [
    "Then we obtain M2M tokens to execute the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4b2bfe-8ec6-4d6e-80d8-d1a047d0a185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:23:04.329112Z",
     "iopub.status.busy": "2023-01-09T16:23:04.328770Z",
     "iopub.status.idle": "2023-01-09T16:23:05.028789Z",
     "shell.execute_reply": "2023-01-09T16:23:05.027169Z",
     "shell.execute_reply.started": "2023-01-09T16:23:04.329084Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200 OK]>\n",
      "<Response [200 OK]>\n"
     ]
    }
   ],
   "source": [
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_ADMIN}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_ADMIN}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "admin_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)\n",
    "\n",
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_UPDATER}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_UPDATER}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "updater_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2977b5e-e3fe-4947-9f33-918e130f57eb",
   "metadata": {},
   "source": [
    "Now we get some info from the backend so we know what's already in the database.\n",
    "If the database has been re-provisioned, this may come back empty. If that happens, don't worry and proceed to the next step where the cause for this issue will be rectified.\n",
    "\n",
    "We'll do a quick check for `categories` but you can also check `languages`, `indicators` or any other top-level element in the `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a726e7-34fc-4625-890c-4ab07e76c638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:23:48.411568Z",
     "iopub.status.busy": "2023-01-09T16:23:48.411207Z",
     "iopub.status.idle": "2023-01-09T16:23:48.727273Z",
     "shell.execute_reply": "2023-01-09T16:23:48.726634Z",
     "shell.execute_reply.started": "2023-01-09T16:23:48.411535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categories\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aa5b5-feac-44f7-b26a-852507aa7cce",
   "metadata": {},
   "source": [
    "If any of categories, indicators, spatial or temporal resolutions are missing, we need to load the config first and then repeat the data retrieval.\n",
    "\n",
    "Since the payload can get quite large, we'll compress it before sending it to the API. The backend API supports both compressed and uncompressed requests; provided you set the appropriate encoding in the header:\n",
    "\n",
    "```python\n",
    "headers={\n",
    "    # always send the type\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # encoding required for gzip-compressed payloads\n",
    "    \"Content-Encoding\": \"gzip\",\n",
    "    [...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de63f9f-22e7-430a-9865-b0eb69dd3a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:23:58.045335Z",
     "iopub.status.busy": "2023-01-09T16:23:58.045063Z",
     "iopub.status.idle": "2023-01-09T16:24:01.839975Z",
     "shell.execute_reply": "2023-01-09T16:24:01.839367Z",
     "shell.execute_reply.started": "2023-01-09T16:23:58.045318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204: <no content>\n"
     ]
    }
   ],
   "source": [
    "# get config directly from the resources\n",
    "with open(f\"{os.getenv('PACKAGE_NAME')}/src/impl/resources/config.json\") as json_data:\n",
    "    config = json.load(json_data)\n",
    "\n",
    "response = httpx.post(\n",
    "    url=f\"{BASE_URL}/setup\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Content-Encoding\": \"gzip\",\n",
    "        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    },\n",
    "    data=gzip.compress(json.dumps(config).encode(\"utf-8\")),\n",
    "    timeout=3600,\n",
    ")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98cf8f-9f04-422b-a6ba-393e54059715",
   "metadata": {},
   "source": [
    "Either way, the db should now have a basic setup.\n",
    "Let's check if we have all the metadata we need before we proceed.\n",
    "While we're at it, we save the categories so we can use them for the ingestion in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3971755-2d36-44c4-a36a-f2d612fd2642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:24:04.442710Z",
     "iopub.status.busy": "2023-01-09T16:24:04.442172Z",
     "iopub.status.idle": "2023-01-09T16:24:04.657534Z",
     "shell.execute_reply": "2023-01-09T16:24:04.656848Z",
     "shell.execute_reply.started": "2023-01-09T16:24:04.442693Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"category_id\": \"residents\",\n",
      "            \"type\": \"single_location\",\n",
      "            \"order\": 1,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/from-data-to-insights/calculating-mobility-indicators/residents-indicators/\",\n",
      "            \"label\": \"Residents\",\n",
      "            \"description\": \"Residents-class indicators describe long-term (monthly) changes in the number of people whose home location is within each area.\",\n",
      "            \"label_fr\": \"R\\u00e9sidents\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs aux r\\u00e9sidents d\\u00e9crivent les variations (mensuelles) \\u00e0 long terme du nombre de personnes dont le lieu de r\\u00e9sidence se trouve dans chaque zone.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"relocations\",\n",
      "            \"type\": \"flow\",\n",
      "            \"order\": 2,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/from-data-to-insights/calculating-mobility-indicators/relocation-indicators/\",\n",
      "            \"label\": \"Relocation\",\n",
      "            \"description\": \"Relocation-class indicators describe long-term (monthly) changes in the number of people changing their home location between pairs of areas.\",\n",
      "            \"label_fr\": \"Changement de r\\u00e9sidence\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs au changement de r\\u00e9sidence d\\u00e9crivent les variations (mensuelles) \\u00e0 long terme du nombre de personnes qui changent de lieu de r\\u00e9sidence entre des paires de zones.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"visitors\",\n",
      "            \"type\": \"single_location\",\n",
      "            \"order\": 3,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/from-data-to-insights/calculating-mobility-indicators/presence-indicators/\",\n",
      "            \"label\": \"Presence\",\n",
      "            \"description\": \"Presence-class indicators describe short-term (daily) changes in the number of people who are present within each area.\",\n",
      "            \"label_fr\": \"Pr\\u00e9sence journali\\u00e8re\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs \\u00e0 la pr\\u00e9sence d\\u00e9crivent les variations \\u00e0 court terme du nombre de personnes pr\\u00e9sentes dans chaque zone.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"trips\",\n",
      "            \"type\": \"flow\",\n",
      "            \"order\": 4,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/from-data-to-insights/calculating-mobility-indicators/movement-indicators/\",\n",
      "            \"label\": \"Movements\",\n",
      "            \"description\": \"Movements-class indicators describe short-term changes in the number of people who are travelling into, out of and between areas.\",\n",
      "            \"label_fr\": \"Mouvements journaliers\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs aux mouvements d\\u00e9crivent les variations \\u00e0 court terme du nombre de personnes qui se d\\u00e9placent vers, depuis et entre les zones.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e16b77-166c-420d-b5db-227a76721acb",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The data format - although agreed upon in principle - is not well-defined enough to guarantee successful insertion.\n",
    "Before we can ingest it, we need to pre-process it in order to\n",
    "\n",
    "- remove empty/invalid data\n",
    "- correctly format the dates\n",
    "- check all expected fields are present\n",
    "- columns are named/ordered correctly\n",
    "- data is sorted (which helps increase ingestion speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e9568f-7066-4be0-992a-5fc6a2aeb1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:24:09.337657Z",
     "iopub.status.busy": "2023-01-09T16:24:09.337449Z",
     "iopub.status.idle": "2023-01-09T16:24:15.303802Z",
     "shell.execute_reply": "2023-01-09T16:24:15.303361Z",
     "shell.execute_reply.started": "2023-01-09T16:24:09.337642Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to /tmp/residents_preprocessed.csv\n",
      "Saved CSV to /tmp/relocations_preprocessed.csv\n",
      "Saved CSV to /tmp/visitors_preprocessed.csv\n",
      "Saved CSV to /tmp/trips_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "files = {\n",
    "    \"residents\": {\"category_id\": \"residents\", \"srid\": 3, \"trid\": 2},\n",
    "    \"relocations\": {\"category_id\": \"relocations\", \"srid\": 3, \"trid\": 2},\n",
    "    \"visitors\": {\"category_id\": \"visitors\", \"srid\": 3, \"trid\": 4},\n",
    "    \"trips\": {\"category_id\": \"trips\", \"srid\": 3, \"trid\": 4},\n",
    "}\n",
    "\n",
    "for file_name in files.keys():\n",
    "    file_path = f\"{os.getenv('PACKAGE_NAME')}/src/impl/resources/data/{file_name}.csv\"\n",
    "    preprocessed_path = f\"/tmp/{file_name}_preprocessed.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # make sure only rows with data are kept\n",
    "    # then sort by date, and spatial unit(s) if applicable\n",
    "    # finally rename columns\n",
    "    if file_name in [\"residents\", \"visitors\"]:\n",
    "        # min columns: date, spatial unit, one data column\n",
    "        df = df.dropna(thresh=3)\n",
    "        df = df.sort_values(by=[df.columns[0], df.columns[1]])\n",
    "        df = df.rename(columns={df.columns[0]: \"date\", df.columns[1]: \"spatial_unit\"})\n",
    "    elif file_name in [\"relocations\", \"trips\"]:\n",
    "        # min columns: date, 2 spatial units, one data column\n",
    "        df = df.dropna(thresh=4)\n",
    "        df = df.sort_values(by=[df.columns[0], df.columns[1], df.columns[2]])\n",
    "        df = df.rename(\n",
    "            columns={df.columns[0]: \"date\", df.columns[1]: \"origin\", df.columns[1]: \"destination\"}\n",
    "        )\n",
    "\n",
    "    df.to_csv(preprocessed_path, index=False)\n",
    "    print(f\"Saved CSV to {preprocessed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7396ae-a7b9-4ffd-a244-dad08121cd61",
   "metadata": {},
   "source": [
    "We can check the files before starting the ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766b7455-ed80-442d-bed6-dc0faa739699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:24:35.380859Z",
     "iopub.status.busy": "2023-01-09T16:24:35.380626Z",
     "iopub.status.idle": "2023-01-09T16:24:35.416986Z",
     "shell.execute_reply": "2023-01-09T16:24:35.416578Z",
     "shell.execute_reply.started": "2023-01-09T16:24:35.380843Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7030 /tmp/residents_preprocessed.csv\n",
      "--------------------\n",
      "date,spatial_unit,residents,residents_perKm2,arrived,departed,delta_arrived,residents_diffwithref,abnormality,residents_pctchangewithref\n",
      "2020-02-01,HT0111-01,522200,28430,27050,22470,4580,-23330,-3.21,-4.28\n",
      "2020-02-01,HT0111-02,73160,10560,4940,5590,-650,-240,-0.06,-0.33\n",
      "--------------------\n",
      "2022-02-01,HT0934-02,28470,270,2570,590,1980,1700,0.79,6.35\n",
      "2022-02-01,HT0934-05,25380,830,430,500,-70,80,0.0,0.32\n",
      "2022-02-01,HT0934-07,16480,350,510,530,-20,-1970,-13.42,-10.67\n"
     ]
    }
   ],
   "source": [
    "%%bash -s /tmp/residents_preprocessed.csv\n",
    "# check the file\n",
    "wc -l \"$1\"\n",
    "echo '--------------------'\n",
    "head -n 3 \"$1\"\n",
    "echo '--------------------'\n",
    "tail -n 3 \"$1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d0db6-9296-431b-a9aa-a22c6a1c360e",
   "metadata": {},
   "source": [
    "## Now we can load the data we want to ingest.\n",
    "We need to process the CSV files, which contain all indicators and dates for one category but the API ingests data in smaller chunks (one API call per indicator per date).\n",
    "It's still recommended to compress the request body using `gzip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb65dfb-5e73-4cb4-8cbb-0339f6d9752f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T16:24:43.742734Z",
     "iopub.status.busy": "2023-01-09T16:24:43.742429Z",
     "iopub.status.idle": "2023-01-09T16:40:48.306328Z",
     "shell.execute_reply": "2023-01-09T16:40:48.305867Z",
     "shell.execute_reply.started": "2023-01-09T16:24:43.742709Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file residents.csv\n",
      "Found 8 indicators for category residents\n",
      "Prepared requests in 2.16s\n",
      "Starting ingestion of data for 8 indicators...\n",
      "    Ingesting 23 datasets for indicator residents.residents.\n",
      "    Ingesting 23 datasets for indicator residents.residents_perKm2.\n",
      "    Ingesting 23 datasets for indicator residents.arrived.\n",
      "    Ingesting 23 datasets for indicator residents.departed.\n",
      "    Ingesting 23 datasets for indicator residents.delta_arrived.\n",
      "    Ingesting 23 datasets for indicator residents.residents_diffwithref.\n",
      "    Ingesting 23 datasets for indicator residents.abnormality.\n",
      "    Ingesting 23 datasets for indicator residents.residents_pctchangewithref.\n",
      "Ingested 184/184 datasets (100.0%) in 6.81s\n",
      "Processing file relocations.csv\n",
      "Found 4 indicators for category relocations\n",
      "Prepared requests in 3.27s\n",
      "Starting ingestion of data for 4 indicators...\n",
      "    Ingesting 23 datasets for indicator relocations.relocations.\n",
      "    Ingesting 23 datasets for indicator relocations.relocations_diffwithref.\n",
      "    Ingesting 23 datasets for indicator relocations.abnormality.\n",
      "    Ingesting 23 datasets for indicator relocations.relocations_pctchangewithref.\n",
      "Ingested 92/92 datasets (100.0%) in 10.3s\n",
      "Processing file visitors.csv\n",
      "Found 7 indicators for category visitors\n",
      "Prepared requests in 60.83s\n",
      "Starting ingestion of data for 7 indicators...\n",
      "    Ingesting 727 datasets for indicator visitors.visitors...............\n",
      "    Ingesting 727 datasets for indicator visitors.visitors_perKm2...............\n",
      "    Ingesting 667 datasets for indicator visitors.trips_in..............\n",
      "    Ingesting 667 datasets for indicator visitors.trips_out..............\n",
      "    Ingesting 727 datasets for indicator visitors.abnormality...............\n",
      "    Ingesting 727 datasets for indicator visitors.visitors_diffwithref...............\n",
      "    Ingesting 727 datasets for indicator visitors.visitors_pctchangewithref...............\n",
      "Ingested 4969/4969 datasets (100.0%) in 183.38s\n",
      "Processing file trips.csv\n",
      "Found 4 indicators for category trips\n",
      "Prepared requests in 270.43s\n",
      "Starting ingestion of data for 4 indicators...\n",
      "    Ingesting 667 datasets for indicator trips.travellers..............\n",
      "    Ingesting 667 datasets for indicator trips.abnormality..............\n",
      "    Ingesting 667 datasets for indicator trips.travellers_diffwithref..............\n",
      "    Ingesting 667 datasets for indicator trips.travellers_pctchangewithref..............\n",
      "Ingested 2668/2668 datasets (100.0%) in 425.76s\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 50\n",
    "\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "async def post_async(ds, client):\n",
    "    return await client.post(\n",
    "        url=f\"{BASE_URL}/data\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Content-Encoding\": \"gzip\",\n",
    "            \"Authorization\": f\"Bearer {admin_token}\",\n",
    "        },\n",
    "        data=gzip.compress(json.dumps(ds, default=str).encode(\"utf-8\")),\n",
    "        timeout=3600,\n",
    "    )\n",
    "\n",
    "\n",
    "async def ingest_data(file_name):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    print(f\"Processing file {file_name}.csv\")\n",
    "    f = files[file_name]\n",
    "\n",
    "    # get the category object\n",
    "    category_id = f[\"category_id\"]\n",
    "    category = [c for c in categories if c[\"category_id\"] == category_id][0]\n",
    "    # get all indicators for that category\n",
    "    response = httpx.get(\n",
    "        url=f\"{BASE_URL}/indicators_for_category/{category_id}\",\n",
    "        headers={\"Authorization\": f\"Bearer {admin_token}\"},\n",
    "    )\n",
    "    indicators = json.loads(response.content)[\"indicators\"]\n",
    "    print(f\"Found {len(indicators)} indicators for category {category_id}\")\n",
    "\n",
    "    # get column names and order from csv\n",
    "    columns = []\n",
    "    column_to_indicator_id = {}\n",
    "\n",
    "    with open(f\"/tmp/{file_name}_preprocessed.csv\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        for row in csv_reader:\n",
    "            columns = row\n",
    "            break\n",
    "\n",
    "        for col in columns:\n",
    "            column_to_indicator_id[col] = f\"{category_id}.{col}\"\n",
    "\n",
    "        # sort data by ID for easier processing\n",
    "        indicators_by_id = {i[\"indicator_id\"]: i for i in indicators}\n",
    "\n",
    "        # collect all datasets in a dict by indicator ID and date - that way it doesn't matter whether the CSV file is ordered\n",
    "        datasets = {}\n",
    "        total_num = 0\n",
    "        # we already consumed the first row so can continue here with the same reader\n",
    "        for row in csv_reader:\n",
    "            # check each column in each row\n",
    "            for col in columns:\n",
    "                if (\n",
    "                    col in column_to_indicator_id\n",
    "                    and column_to_indicator_id[col] in indicators_by_id\n",
    "                ):\n",
    "                    value = row[columns.index(col)]\n",
    "                    # skip \"None\" values\n",
    "                    if value in [\"NaN\", \"Inf\", \"-Inf\", \"\"]:\n",
    "                        continue\n",
    "\n",
    "                    # make sure to use the correct row for the date and the correct datetime format\n",
    "                    date_string = row[0]\n",
    "                    dt = parser.parse(date_string)\n",
    "\n",
    "                    # not one per indicator but one per indicator per date\n",
    "                    indicator = indicators_by_id[column_to_indicator_id[col]]\n",
    "                    indicator_id = indicator[\"indicator_id\"]\n",
    "                    datasets.setdefault(indicator_id, {})\n",
    "\n",
    "                    if date_string not in datasets[indicator_id]:\n",
    "                        total_num += 1\n",
    "\n",
    "                    datasets[indicator_id].setdefault(\n",
    "                        date_string,\n",
    "                        {\n",
    "                            \"metadata\": {\n",
    "                                \"revision\": \"v0.1-demo\",\n",
    "                                # adding a date here which will be overwritten later when it is actually added to the db\n",
    "                                # this is to avoid a fastapi.exceptions.RequestValidationError for checking the length of a \"None\" type\n",
    "                                \"date_added\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                                \"category_id\": category_id,\n",
    "                                \"indicator_id\": indicator_id,\n",
    "                                \"srid\": f[\"srid\"],\n",
    "                                \"trid\": f[\"trid\"],\n",
    "                                \"dt\": dt,\n",
    "                            },\n",
    "                            \"data_type\": category[\"type\"],\n",
    "                            \"data_input\": [],\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "                    datasets[indicator_id][date_string][\"data_input\"].append(\n",
    "                        {\n",
    "                            \"spatial_unit_ids\": [row[1]]\n",
    "                            if category[\"type\"] == \"single_location\"\n",
    "                            else [row[1], row[2]],\n",
    "                            \"data\": value,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    print(f\"Prepared requests in {round(time.monotonic() - start_time, 2)}s\")\n",
    "    print(f\"Starting ingestion of data for {len(datasets)} indicators...\")\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        start_time = time.monotonic()\n",
    "        num = 0\n",
    "        for indicator_id in datasets:\n",
    "            print(\n",
    "                f\"    Ingesting {len(datasets[indicator_id].values())} datasets for indicator {indicator_id}\",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "            for chunk in chunked_iterable(datasets[indicator_id].values(), size=CHUNK_SIZE):\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                responses = await asyncio.gather(*(post_async(ds, client) for ds in chunk))\n",
    "                for response in responses:\n",
    "                    if response.status_code not in [201, 204]:\n",
    "                        print(\"\")\n",
    "                        log(response)\n",
    "                    else:\n",
    "                        num += 1\n",
    "            print(\"\")\n",
    "\n",
    "        print(\n",
    "            f\"Ingested {num}/{total_num} datasets ({round(num/total_num*100, 2)}%) in {round(time.monotonic() - start_time, 2)}s\"\n",
    "        )\n",
    "\n",
    "\n",
    "for file_name in files.keys():\n",
    "    # if file_name != 'residents':\n",
    "    #    continue\n",
    "    await ingest_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbcf12-d4b3-4328-ae27-39e1aa4c6392",
   "metadata": {},
   "source": [
    "Done! Provided you got all `201` or `204` responses (i.e. no errors), the data should now be in the database!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 13 2021, 10:38:41) \n[GCC 10.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c180f8ebda99849f742c1970511da7e002d61838aeb8176b3be05803a158b625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
