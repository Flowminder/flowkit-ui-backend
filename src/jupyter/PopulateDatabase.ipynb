{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436ca00e-6bc0-4bb6-9134-ece22fb241c8",
   "metadata": {},
   "source": [
    "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0.\n",
    "\n",
    "If a copy of the MPL was not distributed with this file, You can obtain one at https://mozilla.org/MPL/2.0/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bf962-019c-40a8-9996-9cf366fd7c50",
   "metadata": {},
   "source": [
    "# Populating the database\n",
    "\n",
    "This notebook will guide you through the process of adding data to the database.\n",
    "\n",
    "First we import the required libraries and check the connection works.\n",
    "\n",
    "**Note for CGP**:\n",
    "Because containers are spun up only when \"poked\", you may need to run this twice in order to give the container time to spin up if you receive a `TimeoutError` after making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92716416-e32a-4944-9af2-23b9ac53d576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:23:45.820275Z",
     "iopub.status.busy": "2023-03-17T15:23:45.820072Z",
     "iopub.status.idle": "2023-03-17T15:23:46.254548Z",
     "shell.execute_reply": "2023-03-17T15:23:46.253849Z",
     "shell.execute_reply.started": "2023-03-17T15:23:45.820232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to flowkit-ui-backend!\n",
      "{\n",
      "    \"datetime\": \"2023-03-17T15:23:46.249913+00:00\",\n",
      "    \"docker_image\": \"flowminder/flowkit-ui-backend:890c0e0\",\n",
      "    \"git_branch\": \"feature-scope-mapping\",\n",
      "    \"git_commit\": \"890c0e0\",\n",
      "    \"git_tag\": null,\n",
      "    \"python_package\": \"flowkit-ui-backend\",\n",
      "    \"python_version\": \"3.9.15\",\n",
      "    \"api_version_url_appendix\": \"v1\",\n",
      "    \"api_version\": \"1.2.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import asyncio\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# whether to ingest synthetic data or real data\n",
    "SYNTHETIC = True\n",
    "\n",
    "AUDIENCE = os.getenv(\"AUTH0_AUDIENCE\")\n",
    "BASE_URL = os.getenv(\"INGESTION_BASE_URL\")\n",
    "AUTH0_DOMAIN = os.getenv(\"INGESTION_AUTH0_DOMAIN\")\n",
    "AUTH0_CLIENT_ID_UPDATER = os.getenv(\"INGESTION_AUTH0_CLIENT_ID_UPDATER\")\n",
    "AUTH0_CLIENT_SECRET_UPDATER = os.getenv(\"INGESTION_AUTH0_CLIENT_SECRET_UPDATER\")\n",
    "AUTH0_CLIENT_ID_ADMIN = os.getenv(\"INGESTION_AUTH0_CLIENT_ID_ADMIN\")\n",
    "AUTH0_CLIENT_SECRET_ADMIN = os.getenv(\"INGESTION_AUTH0_CLIENT_SECRET_ADMIN\")\n",
    "\n",
    "\n",
    "def log(response):\n",
    "    to_print = f\"{response.status_code}: \" if response.status_code != 200 else \"\"\n",
    "    if hasattr(response, \"content\") and response.content is not None and response.content != b\"\":\n",
    "        try:\n",
    "            to_print += json.dumps(json.loads(response.content), indent=4)\n",
    "        except Exception as e:\n",
    "            to_print += \"<could not decode response>\"\n",
    "    else:\n",
    "        to_print += \"<no content>\"\n",
    "    print(to_print)\n",
    "\n",
    "\n",
    "response = httpx.get(f\"{BASE_URL}/heartbeat\")\n",
    "\n",
    "print(f\"Welcome to {os.getenv('APP_NAME')}!\")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf8c47-3647-429f-9a0c-38968c4c0b72",
   "metadata": {},
   "source": [
    "Then we obtain M2M tokens to execute the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4b2bfe-8ec6-4d6e-80d8-d1a047d0a185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:23:48.351557Z",
     "iopub.status.busy": "2023-03-17T15:23:48.351220Z",
     "iopub.status.idle": "2023-03-17T15:23:49.554108Z",
     "shell.execute_reply": "2023-03-17T15:23:49.553565Z",
     "shell.execute_reply.started": "2023-03-17T15:23:48.351530Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200 OK]>\n",
      "<Response [200 OK]>\n"
     ]
    }
   ],
   "source": [
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_ADMIN}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_ADMIN}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "admin_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)\n",
    "\n",
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_UPDATER}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_UPDATER}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "updater_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2977b5e-e3fe-4947-9f33-918e130f57eb",
   "metadata": {},
   "source": [
    "Now we get some info from the backend so we know what's already in the database.\n",
    "If the database has been re-provisioned, this may come back empty. If that happens, don't worry and proceed to the next step where the cause for this issue will be rectified.\n",
    "\n",
    "We'll do a quick check for `categories` but you can also check `languages`, `indicators` or any other top-level element in the `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a726e7-34fc-4625-890c-4ab07e76c638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:23:51.313094Z",
     "iopub.status.busy": "2023-03-17T15:23:51.312852Z",
     "iopub.status.idle": "2023-03-17T15:23:51.737572Z",
     "shell.execute_reply": "2023-03-17T15:23:51.737064Z",
     "shell.execute_reply.started": "2023-03-17T15:23:51.313081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categories\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aa5b5-feac-44f7-b26a-852507aa7cce",
   "metadata": {},
   "source": [
    "If any of categories, indicators, spatial or temporal resolutions are missing, we need to load the config first and then repeat the data retrieval.\n",
    "\n",
    "Since the payload can get quite large, we'll compress it before sending it to the API. The backend API supports both compressed and uncompressed requests; provided you set the appropriate encoding in the header:\n",
    "\n",
    "```python\n",
    "headers={\n",
    "    # always send the type\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # encoding required for gzip-compressed payloads\n",
    "    \"Content-Encoding\": \"gzip\",\n",
    "    [...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de63f9f-22e7-430a-9865-b0eb69dd3a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:23:54.545967Z",
     "iopub.status.busy": "2023-03-17T15:23:54.545764Z",
     "iopub.status.idle": "2023-03-17T15:23:56.950909Z",
     "shell.execute_reply": "2023-03-17T15:23:56.950090Z",
     "shell.execute_reply.started": "2023-03-17T15:23:54.545951Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204: <no content>\n"
     ]
    }
   ],
   "source": [
    "# get config directly from the resources\n",
    "with open(f\"{os.getenv('PACKAGE_NAME')}/src/impl/resources/config.json\") as json_data:\n",
    "    config = json.load(json_data)\n",
    "\n",
    "response = httpx.post(\n",
    "    url=f\"{BASE_URL}/setup\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Content-Encoding\": \"gzip\",\n",
    "        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    },\n",
    "    data=gzip.compress(json.dumps(config).encode(\"utf-8\")),\n",
    "    timeout=3600,\n",
    ")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98cf8f-9f04-422b-a6ba-393e54059715",
   "metadata": {},
   "source": [
    "Either way, the db should now have a basic setup.\n",
    "Let's check if we have all the metadata we need before we proceed.\n",
    "While we're at it, we save the categories so we can use them for the ingestion in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3971755-2d36-44c4-a36a-f2d612fd2642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:23:59.523702Z",
     "iopub.status.busy": "2023-03-17T15:23:59.523486Z",
     "iopub.status.idle": "2023-03-17T15:23:59.539345Z",
     "shell.execute_reply": "2023-03-17T15:23:59.538752Z",
     "shell.execute_reply.started": "2023-03-17T15:23:59.523687Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"category_id\": \"residents\",\n",
      "            \"type\": \"single_location\",\n",
      "            \"order\": 1,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/methods/calculating-mobility-indicators/residents-indicators\",\n",
      "            \"label\": \"Residents\",\n",
      "            \"description\": \"Residents-class indicators describe long-term (monthly) changes in the number of people whose home location is within each area.\",\n",
      "            \"label_fr\": \"R\\u00e9sidents\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs aux r\\u00e9sidents d\\u00e9crivent les variations (mensuelles) \\u00e0 long terme du nombre de personnes dont le lieu de r\\u00e9sidence se trouve dans chaque zone.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"relocations\",\n",
      "            \"type\": \"flow\",\n",
      "            \"order\": 2,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/methods/calculating-mobility-indicators/relocation-indicators/\",\n",
      "            \"label\": \"Relocation\",\n",
      "            \"description\": \"Relocation-class indicators describe long-term (monthly) changes in the number of people changing their home location between pairs of areas.\",\n",
      "            \"label_fr\": \"Changement de r\\u00e9sidence\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs au changement de r\\u00e9sidence d\\u00e9crivent les variations (mensuelles) \\u00e0 long terme du nombre de personnes qui changent de lieu de r\\u00e9sidence entre des paires de zones.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"presence\",\n",
      "            \"type\": \"single_location\",\n",
      "            \"order\": 3,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/methods/calculating-mobility-indicators/presence-indicators/\",\n",
      "            \"label\": \"Presence\",\n",
      "            \"description\": \"Presence-class indicators describe short-term (daily) changes in the number of people who are present within each area.\",\n",
      "            \"label_fr\": \"Pr\\u00e9sence journali\\u00e8re\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs \\u00e0 la pr\\u00e9sence d\\u00e9crivent les variations \\u00e0 court terme du nombre de personnes pr\\u00e9sentes dans chaque zone.\"\n",
      "        },\n",
      "        {\n",
      "            \"category_id\": \"movements\",\n",
      "            \"type\": \"flow\",\n",
      "            \"order\": 4,\n",
      "            \"flowgeek_url\": \"https://www.flowgeek.org/methods/calculating-mobility-indicators/movement-indicators\",\n",
      "            \"label\": \"Movements\",\n",
      "            \"description\": \"Movements-class indicators describe short-term changes in the number of people who are travelling into, out of and between areas.\",\n",
      "            \"label_fr\": \"Mouvements journaliers\",\n",
      "            \"description_fr\": \"Les indicateurs relatifs aux mouvements d\\u00e9crivent les variations \\u00e0 court terme du nombre de personnes qui se d\\u00e9placent vers, depuis et entre les zones.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e16b77-166c-420d-b5db-227a76721acb",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The data format - although agreed upon in principle - is not well-defined enough to guarantee successful insertion.\n",
    "Before we can ingest it, we need to pre-process it in order to\n",
    "\n",
    "- remove empty/invalid data\n",
    "- correctly format the dates\n",
    "- check all expected fields are present\n",
    "- columns are named/ordered correctly\n",
    "- data is sorted (which helps increase ingestion speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e9568f-7066-4be0-992a-5fc6a2aeb1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:24:03.097999Z",
     "iopub.status.busy": "2023-03-17T15:24:03.097823Z",
     "iopub.status.idle": "2023-03-17T15:24:03.283638Z",
     "shell.execute_reply": "2023-03-17T15:24:03.282830Z",
     "shell.execute_reply.started": "2023-03-17T15:24:03.097987Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to /tmp/residents_admin3_monthly_small_preprocessed.csv\n",
      "Saved CSV to /tmp/relocations_admin3_monthly_small_preprocessed.csv\n",
      "Saved CSV to /tmp/presence_admin3_daily_small_preprocessed.csv\n",
      "Saved CSV to /tmp/movements_admin3_daily_small_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "real_data_files = {\n",
    "    \"residents\": {\"category_id\": \"residents\", \"srid\": 3, \"trid\": 2},\n",
    "    \"relocations\": {\"category_id\": \"relocations\", \"srid\": 3, \"trid\": 2},\n",
    "    \"presence\": {\"category_id\": \"presence\", \"srid\": 3, \"trid\": 4},\n",
    "    \"movements\": {\"category_id\": \"movements\", \"srid\": 3, \"trid\": 4},\n",
    "}\n",
    "synthetic_files = {\n",
    "    \"residents_admin3_monthly_small\": {\"category_id\": \"residents\", \"srid\": 3, \"trid\": 2},\n",
    "    \"relocations_admin3_monthly_small\": {\"category_id\": \"relocations\", \"srid\": 3, \"trid\": 2},\n",
    "    \"presence_admin3_daily_small\": {\"category_id\": \"presence\", \"srid\": 3, \"trid\": 4},\n",
    "    \"movements_admin3_daily_small\": {\"category_id\": \"movements\", \"srid\": 3, \"trid\": 4},\n",
    "}\n",
    "files = synthetic_files if SYNTHETIC else real_data_files\n",
    "\n",
    "parent_dir = f\"{os.getenv('PACKAGE_NAME')}/src/impl/resources\"\n",
    "data_dir = f\"{parent_dir}/data/synthetic\" if SYNTHETIC else f\"{parent_dir}/data\"\n",
    "\n",
    "for file_name in files.keys():\n",
    "    file_path = f\"{data_dir}/{file_name}.csv\"\n",
    "    preprocessed_path = f\"/tmp/{file_name}_preprocessed.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # make sure only rows with data are kept\n",
    "    # then sort by date, and spatial unit(s) if applicable\n",
    "    # finally rename columns\n",
    "    if file_name in [\"residents\", \"presence\"]:\n",
    "        # min columns: date, spatial unit, one data column\n",
    "        df = df.dropna(thresh=3)\n",
    "        df = df.sort_values(by=[df.columns[0], df.columns[1]])\n",
    "        df = df.rename(columns={df.columns[0]: \"date\", df.columns[1]: \"spatial_unit\"})\n",
    "    elif file_name in [\"relocations\", \"movements\"]:\n",
    "        # min columns: date, 2 spatial units, one data column\n",
    "        df = df.dropna(thresh=4)\n",
    "        df = df.sort_values(by=[df.columns[0], df.columns[1], df.columns[2]])\n",
    "        df = df.rename(\n",
    "            columns={df.columns[0]: \"date\", df.columns[1]: \"origin\", df.columns[1]: \"destination\"}\n",
    "        )\n",
    "\n",
    "    df.to_csv(preprocessed_path, index=False)\n",
    "    print(f\"Saved CSV to {preprocessed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7396ae-a7b9-4ffd-a244-dad08121cd61",
   "metadata": {},
   "source": [
    "We can check the files before starting the ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766b7455-ed80-442d-bed6-dc0faa739699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:24:05.076536Z",
     "iopub.status.busy": "2023-03-17T15:24:05.076259Z",
     "iopub.status.idle": "2023-03-17T15:24:05.086435Z",
     "shell.execute_reply": "2023-03-17T15:24:05.085937Z",
     "shell.execute_reply.started": "2023-03-17T15:24:05.076521Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3129 /tmp/residents_admin3_monthly_small_preprocessed.csv\n",
      "--------------------\n",
      "month,pcod,residents,residents_perKm2,arrived,departed,delta_arrived,residents_diffwithref,abnormality,residents_pctchangewithref\n",
      "2020-02-01,HT0111-01,544650,29650,446860,427110,19760,-880.0,-0.68,-0.16\n",
      "2020-02-01,HT0111-02,75340,10880,33420,31480,1940,1950.0,0.86,2.65\n",
      "--------------------\n",
      "2020-07-01,HT1031-05,9630,170,1040,1040,0,0.0,0.0,0.0\n",
      "2020-07-01,HT1032-01,1770,60,160,160,0,0.0,0.0,0.0\n",
      "2020-07-01,HT1032-02,3030,220,420,420,0,0.0,0.0,0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash -s /tmp/residents_admin3_monthly_small_preprocessed.csv\n",
    "# check the file\n",
    "wc -l \"$1\"\n",
    "echo '--------------------'\n",
    "head -n 3 \"$1\"\n",
    "echo '--------------------'\n",
    "tail -n 3 \"$1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d0db6-9296-431b-a9aa-a22c6a1c360e",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Now we can load the data we want to ingest.\n",
    "We need to process the CSV files, which contain all indicators and dates for one category but the API ingests data in smaller chunks (one API call per indicator per date).\n",
    "It's still recommended to compress the request body using `gzip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb65dfb-5e73-4cb4-8cbb-0339f6d9752f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:24:06.964178Z",
     "iopub.status.busy": "2023-03-17T15:24:06.963903Z",
     "iopub.status.idle": "2023-03-17T15:24:27.616547Z",
     "shell.execute_reply": "2023-03-17T15:24:27.615909Z",
     "shell.execute_reply.started": "2023-03-17T15:24:06.964161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file residents_admin3_monthly_small.csv\n",
      "Found 8 indicators for category residents\n",
      "Prepared requests in 0.72s\n",
      "Starting ingestion of data for 8 indicators...\n",
      "    Ingesting 6 datasets for indicator residents.residents.\n",
      "    Ingesting 6 datasets for indicator residents.residents_perKm2.\n",
      "    Ingesting 6 datasets for indicator residents.arrived.\n",
      "    Ingesting 6 datasets for indicator residents.departed.\n",
      "    Ingesting 6 datasets for indicator residents.delta_arrived.\n",
      "    Ingesting 6 datasets for indicator residents.residents_diffwithref.\n",
      "    Ingesting 6 datasets for indicator residents.abnormality.\n",
      "    Ingesting 6 datasets for indicator residents.residents_pctchangewithref.\n",
      "Ingested 48/48 datasets (100.0%) in 1.71s\n",
      "Processing file relocations_admin3_monthly_small.csv\n",
      "Found 4 indicators for category relocations\n",
      "Prepared requests in 3.52s\n",
      "Starting ingestion of data for 4 indicators...\n",
      "    Ingesting 6 datasets for indicator relocations.relocations.\n",
      "    Ingesting 6 datasets for indicator relocations.relocations_diffwithref.\n",
      "    Ingesting 6 datasets for indicator relocations.abnormality.\n",
      "    Ingesting 6 datasets for indicator relocations.relocations_pctchangewithref.\n",
      "Ingested 24/24 datasets (100.0%) in 4.51s\n",
      "Processing file presence_admin3_daily_small.csv\n",
      "Found 7 indicators for category presence\n",
      "Prepared requests in 0.63s\n",
      "Starting ingestion of data for 7 indicators...\n",
      "    Ingesting 7 datasets for indicator presence.presence.\n",
      "    Ingesting 7 datasets for indicator presence.presence_perKm2.\n",
      "    Ingesting 7 datasets for indicator presence.trips_in.\n",
      "    Ingesting 7 datasets for indicator presence.trips_out.\n",
      "    Ingesting 7 datasets for indicator presence.abnormality.\n",
      "    Ingesting 7 datasets for indicator presence.presence_diffwithref.\n",
      "    Ingesting 7 datasets for indicator presence.presence_pctchangewithref.\n",
      "Ingested 49/49 datasets (100.0%) in 1.63s\n",
      "Processing file movements_admin3_daily_small.csv\n",
      "Found 4 indicators for category movements\n",
      "Prepared requests in 3.42s\n",
      "Starting ingestion of data for 4 indicators...\n",
      "    Ingesting 7 datasets for indicator movements.travellers.\n",
      "    Ingesting 7 datasets for indicator movements.abnormality.\n",
      "    Ingesting 7 datasets for indicator movements.travellers_diffwithref.\n",
      "    Ingesting 7 datasets for indicator movements.travellers_pctchangewithref.\n",
      "Ingested 28/28 datasets (100.0%) in 4.41s\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 50\n",
    "\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "async def post_async(ds, client):\n",
    "    return await client.post(\n",
    "        url=f\"{BASE_URL}/data\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Content-Encoding\": \"gzip\",\n",
    "            \"Authorization\": f\"Bearer {admin_token}\",\n",
    "        },\n",
    "        data=gzip.compress(json.dumps(ds, default=str).encode(\"utf-8\")),\n",
    "        timeout=3600,\n",
    "    )\n",
    "\n",
    "\n",
    "async def ingest_data(file_name):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    print(f\"Processing file {file_name}.csv\")\n",
    "    f = files[file_name]\n",
    "\n",
    "    # get the category object\n",
    "    category_id = f[\"category_id\"]\n",
    "    category = [c for c in categories if c[\"category_id\"] == category_id][0]\n",
    "    # get all indicators for that category\n",
    "    response = httpx.get(\n",
    "        url=f\"{BASE_URL}/indicators_for_category/{category_id}\",\n",
    "        headers={\"Authorization\": f\"Bearer {admin_token}\"},\n",
    "    )\n",
    "    indicators = json.loads(response.content)[\"indicators\"]\n",
    "    print(f\"Found {len(indicators)} indicators for category {category_id}\")\n",
    "\n",
    "    # get column names and order from csv\n",
    "    columns = []\n",
    "    column_to_indicator_id = {}\n",
    "\n",
    "    with open(f\"/tmp/{file_name}_preprocessed.csv\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        for row in csv_reader:\n",
    "            columns = row\n",
    "            break\n",
    "\n",
    "        for col in columns:\n",
    "            column_to_indicator_id[col] = f\"{category_id}.{col}\"\n",
    "\n",
    "        # sort data by ID for easier processing\n",
    "        indicators_by_id = {i[\"indicator_id\"]: i for i in indicators}\n",
    "\n",
    "        # collect all datasets in a dict by indicator ID and date - that way it doesn't matter whether the CSV file is ordered\n",
    "        datasets = {}\n",
    "        total_num = 0\n",
    "        # we already consumed the first row so can continue here with the same reader\n",
    "        for row in csv_reader:\n",
    "            # check each column in each row\n",
    "            for col in columns:\n",
    "                if (\n",
    "                    col in column_to_indicator_id\n",
    "                    and column_to_indicator_id[col] in indicators_by_id\n",
    "                ):\n",
    "                    value = row[columns.index(col)]\n",
    "                    # skip \"None\" values\n",
    "                    if value in [\"NaN\", \"Inf\", \"-Inf\", \"\"]:\n",
    "                        continue\n",
    "\n",
    "                    # make sure to use the correct row for the date and the correct datetime format\n",
    "                    date_string = row[0]\n",
    "                    dt = parser.parse(date_string)\n",
    "\n",
    "                    # not one per indicator but one per indicator per date\n",
    "                    indicator = indicators_by_id[column_to_indicator_id[col]]\n",
    "                    indicator_id = indicator[\"indicator_id\"]\n",
    "                    datasets.setdefault(indicator_id, {})\n",
    "\n",
    "                    if date_string not in datasets[indicator_id]:\n",
    "                        total_num += 1\n",
    "\n",
    "                    datasets[indicator_id].setdefault(\n",
    "                        date_string,\n",
    "                        {\n",
    "                            \"metadata\": {\n",
    "                                \"revision\": \"v0.1-demo\",\n",
    "                                # adding a date here which will be overwritten later when it is actually added to the db\n",
    "                                # this is to avoid a fastapi.exceptions.RequestValidationError for checking the length of a \"None\" type\n",
    "                                \"date_added\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                                \"category_id\": category_id,\n",
    "                                \"indicator_id\": indicator_id,\n",
    "                                \"srid\": f[\"srid\"],\n",
    "                                \"trid\": f[\"trid\"],\n",
    "                                \"dt\": dt,\n",
    "                            },\n",
    "                            \"data_type\": category[\"type\"],\n",
    "                            \"data_input\": [],\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "                    datasets[indicator_id][date_string][\"data_input\"].append(\n",
    "                        {\n",
    "                            \"spatial_unit_ids\": [row[1]]\n",
    "                            if category[\"type\"] == \"single_location\"\n",
    "                            else [row[1], row[2]],\n",
    "                            \"data\": value,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    print(f\"Prepared requests in {round(time.monotonic() - start_time, 2)}s\")\n",
    "    print(f\"Starting ingestion of data for {len(datasets)} indicators...\")\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        start_time = time.monotonic()\n",
    "        num = 0\n",
    "        for indicator_id in datasets:\n",
    "            print(\n",
    "                f\"    Ingesting {len(datasets[indicator_id].values())} datasets for indicator {indicator_id}\",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "            for chunk in chunked_iterable(datasets[indicator_id].values(), size=CHUNK_SIZE):\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                responses = await asyncio.gather(*(post_async(ds, client) for ds in chunk))\n",
    "                for response in responses:\n",
    "                    if response.status_code not in [201, 204]:\n",
    "                        print(\"\")\n",
    "                        log(response)\n",
    "                    else:\n",
    "                        num += 1\n",
    "            print(\"\")\n",
    "\n",
    "        print(\n",
    "            f\"Ingested {num}/{total_num} datasets ({round(num/total_num*100, 2)}%) in {round(time.monotonic() - start_time, 2)}s\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def doit():\n",
    "    for file_name in files.keys():\n",
    "        # if file_name != 'residents':\n",
    "        #    continue\n",
    "        await ingest_data(file_name)\n",
    "\n",
    "await doit()\n",
    "#loop = asyncio.get_event_loop()\n",
    "#task = loop.create_task(doit())\n",
    "#if not loop.is_running():\n",
    "#    loop.run_until_complete(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbcf12-d4b3-4328-ae27-39e1aa4c6392",
   "metadata": {},
   "source": [
    "Done! Provided you got all `201` or `204` responses (i.e. no errors), the data should now be in the database!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e909784-5a22-4105-93da-48644986ede8",
   "metadata": {},
   "source": [
    "## Data permissions & access\n",
    "\n",
    "The data is now in the database, but without access management, only administrators will be able to see the data by default.\n",
    "To enable access by users depending on their roles, we need to define what scopes give access to which part of the data.\n",
    "We use an \"allow-list\" style access management so we have to define each bit of data that will be accessible to users that aren't admins.\n",
    "We do that using JSON. Each key is the name of a scope as defined in Auth0 (see also the API spec) and each value is a set of queries (as per API spec) that define a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ef4661-f5c1-485e-8600-59ec1a346bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:24:52.784057Z",
     "iopub.status.busy": "2023-03-17T15:24:52.783878Z",
     "iopub.status.idle": "2023-03-17T15:24:52.787592Z",
     "shell.execute_reply": "2023-03-17T15:24:52.787176Z",
     "shell.execute_reply.started": "2023-03-17T15:24:52.784043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_access = {\n",
    "    \"read:free_data\": [\n",
    "        {\n",
    "            \"category_id\": \"residents\",\n",
    "            \"indicator_id\": \"residents.residents\",\n",
    "            \"srid\": 3,\n",
    "            \"trid\": 2,\n",
    "            \"start_date\": \"2020-02\",\n",
    "            \"duration\": 3\n",
    "        }\n",
    "    ],\n",
    "    \"read:premium_data\": [\n",
    "        {\n",
    "            \"category_id\": \"residents\",\n",
    "            \"indicator_id\": \"residents.residents\",\n",
    "            \"srid\": 3,\n",
    "            \"trid\": 2,\n",
    "            \"start_date\": \"2020-02\",\n",
    "            \"duration\": 6\n",
    "        },\n",
    "        {\n",
    "            \"category_id\": \"residents\",\n",
    "            \"indicator_id\": \"residents.residents_perKm2\",\n",
    "            \"srid\": 3,\n",
    "            \"trid\": 2,\n",
    "            \"start_date\": \"2020-02\",\n",
    "            \"duration\": 6\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ac8e8-0491-4b76-a54b-58cf8c03fccb",
   "metadata": {},
   "source": [
    "Now we need to get the metadata IDs of the specified data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85442e50-590c-4665-b45c-eb1579486aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:31:55.868649Z",
     "iopub.status.busy": "2023-03-17T15:31:55.867919Z",
     "iopub.status.idle": "2023-03-17T15:31:55.941132Z",
     "shell.execute_reply": "2023-03-17T15:31:55.940549Z",
     "shell.execute_reply.started": "2023-03-17T15:31:55.868616Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read:free_data': ['3', '1', '4'], 'read:premium_data': ['3', '1', '4', '2', '5', '6', '9', '10', '7', '11', '12', '8']}\n"
     ]
    }
   ],
   "source": [
    "scope_mappings = {}\n",
    "for scope in data_access:\n",
    "    scope_mappings[scope] = []\n",
    "    for query in data_access[scope]:\n",
    "        query['mdids_only'] = True\n",
    "        response = httpx.post(\n",
    "            url=f\"{BASE_URL}/query\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Content-Encoding\": \"gzip\",\n",
    "                \"Authorization\": f\"Bearer {admin_token}\",\n",
    "            },\n",
    "            data=gzip.compress(json.dumps(query).encode(\"utf-8\")),\n",
    "        )\n",
    "        scope_mappings[scope] += json.loads(response.content)[\"mdids\"]\n",
    "print(scope_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af944e3-0e61-4181-8925-01d0a400c446",
   "metadata": {},
   "source": [
    "Next we can ingest the scope mappings using the `/scope_mapping` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de822c81-fca9-4504-b8f1-1c91504855fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T15:35:52.813406Z",
     "iopub.status.busy": "2023-03-17T15:35:52.813193Z",
     "iopub.status.idle": "2023-03-17T15:35:53.465410Z",
     "shell.execute_reply": "2023-03-17T15:35:53.464659Z",
     "shell.execute_reply.started": "2023-03-17T15:35:52.813390Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for scope in scope_mappings:\n",
    "    for mdid in scope_mappings[scope]:\n",
    "        scope_mapping = {\n",
    "            \"scope\": scope,\n",
    "            \"mdid\": mdid\n",
    "        }\n",
    "        response = httpx.post(\n",
    "            url=f\"{BASE_URL}/scope_mapping\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Content-Encoding\": \"gzip\",\n",
    "                \"Authorization\": f\"Bearer {admin_token}\",\n",
    "            },\n",
    "            data=gzip.compress(json.dumps(scope_mapping).encode(\"utf-8\")),\n",
    "        )\n",
    "        if response.status_code != 204:\n",
    "            log(response)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360f81a-a785-4e69-8a06-203d712fb526",
   "metadata": {},
   "source": [
    "If not status codes other than `204` come back, then the ingestion of scope mappings worked and the data is now tagged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c180f8ebda99849f742c1970511da7e002d61838aeb8176b3be05803a158b625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
