{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436ca00e-6bc0-4bb6-9134-ece22fb241c8",
   "metadata": {},
   "source": [
    "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0.\n",
    "\n",
    "If a copy of the MPL was not distributed with this file, You can obtain one at https://mozilla.org/MPL/2.0/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bf962-019c-40a8-9996-9cf366fd7c50",
   "metadata": {},
   "source": [
    "# Populating the database\n",
    "\n",
    "This notebook will guide you through the process of adding data to the database.\n",
    "\n",
    "First we import the required libraries and check the connection works.\n",
    "\n",
    "**Note for CGP**:\n",
    "Because containers are spun up only when \"poked\", you may need to run this twice in order to give the container time to spin up if you receive a `TimeoutError` after making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92716416-e32a-4944-9af2-23b9ac53d576",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import asyncio\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# whether to ingest synthetic data or real data\n",
    "SYNTHETIC = False\n",
    "\n",
    "AUDIENCE = os.getenv(\"AUTH0_AUDIENCE\")\n",
    "BASE_URL = os.getenv(\"INGESTION_BASE_URL\")\n",
    "AUTH0_DOMAIN = os.getenv(\"INGESTION_AUTH0_DOMAIN\")\n",
    "AUTH0_CLIENT_ID_UPDATER = os.getenv(\"INGESTION_AUTH0_CLIENT_ID_UPDATER\")\n",
    "AUTH0_CLIENT_SECRET_UPDATER = os.getenv(\"INGESTION_AUTH0_CLIENT_SECRET_UPDATER\")\n",
    "AUTH0_CLIENT_ID_ADMIN = os.getenv(\"INGESTION_AUTH0_CLIENT_ID_ADMIN\")\n",
    "AUTH0_CLIENT_SECRET_ADMIN = os.getenv(\"INGESTION_AUTH0_CLIENT_SECRET_ADMIN\")\n",
    "\n",
    "\n",
    "def log(response):\n",
    "    to_print = f\"{response.status_code}: \" if response.status_code != 200 else \"\"\n",
    "    if hasattr(response, \"content\") and response.content is not None and response.content != b\"\":\n",
    "        try:\n",
    "            to_print += json.dumps(json.loads(response.content), indent=4)\n",
    "        except Exception as e:\n",
    "            to_print += \"<could not decode response>\"\n",
    "    else:\n",
    "        to_print += \"<no content>\"\n",
    "    print(to_print)\n",
    "\n",
    "\n",
    "response = httpx.get(f\"{BASE_URL}/heartbeat\")\n",
    "\n",
    "print(f\"Welcome to {os.getenv('APP_NAME')}!\")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf8c47-3647-429f-9a0c-38968c4c0b72",
   "metadata": {},
   "source": [
    "Then we obtain M2M tokens to execute the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fa850-9ae2-44e2-9629-7efce6b90603",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH0_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b2bfe-8ec6-4d6e-80d8-d1a047d0a185",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_ADMIN}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_ADMIN}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "admin_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)\n",
    "\n",
    "response = httpx.post(\n",
    "    url=f\"https://{AUTH0_DOMAIN}/oauth/token\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=f'{{\"client_id\":\"{AUTH0_CLIENT_ID_UPDATER}\",\"client_secret\":\"{AUTH0_CLIENT_SECRET_UPDATER}\",\"audience\":\"{AUDIENCE}\",\"grant_type\":\"client_credentials\"}}',\n",
    ")\n",
    "updater_token = json.loads(response.content)[\"access_token\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2977b5e-e3fe-4947-9f33-918e130f57eb",
   "metadata": {},
   "source": [
    "Now we get some info from the backend so we know what's already in the database.\n",
    "If the database has been re-provisioned, this may come back empty. If that happens, don't worry and proceed to the next step where the cause for this issue will be rectified.\n",
    "\n",
    "We'll do a quick check for `categories` but you can also check `languages`, `indicators` or any other top-level element in the `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a726e7-34fc-4625-890c-4ab07e76c638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aa5b5-feac-44f7-b26a-852507aa7cce",
   "metadata": {},
   "source": [
    "If any of categories, indicators, spatial or temporal resolutions are missing, we need to load the config first and then repeat the data retrieval.\n",
    "\n",
    "Since the payload can get quite large, we'll compress it before sending it to the API. The backend API supports both compressed and uncompressed requests; provided you set the appropriate encoding in the header:\n",
    "\n",
    "```python\n",
    "headers={\n",
    "    # always send the type\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # encoding required for gzip-compressed payloads\n",
    "    \"Content-Encoding\": \"gzip\",\n",
    "    [...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a5f52-a7a6-471a-adfd-0b4547fec0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get config directly from the resources\n",
    "with open(f\"../impl/resources/config.json\") as json_data:\n",
    "    config = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bf5b6-88f6-457b-af2e-7de3a50882e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = httpx.post(\n",
    "    url=f\"{BASE_URL}/setup\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Content-Encoding\": \"gzip\",\n",
    "        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    },\n",
    "    data=gzip.compress(json.dumps(config).encode(\"utf-8\")),\n",
    "    timeout=3600,\n",
    ")\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98cf8f-9f04-422b-a6ba-393e54059715",
   "metadata": {},
   "source": [
    "Either way, the db should now have a basic setup.\n",
    "Let's check if we have all the metadata we need before we proceed.\n",
    "While we're at it, we save the categories so we can use them for the ingestion in the next step. We'll do the same for the indicators, and spatial and temporal resolutions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3971755-2d36-44c4-a36a-f2d612fd2642",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/categories\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "categories = json.loads(response.content)[\"categories\"]\n",
    "category_type_lookup = {ct[\"category_id\"]: ct[\"type\"] for ct in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51b9e3-969b-4f86-acb7-cdec51712f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/indicators\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "indicators = json.loads(response.content)[\"indicators\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b9612-da09-407c-a3c3-ee4dd63120ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update indicators\n",
    "\n",
    "indicators = config['indicators']\n",
    "for indicator in indicators:\n",
    "    response = httpx.patch(\n",
    "        url=f\"{BASE_URL}/indicators/{indicator['indicator_id']}\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Content-Encoding\": \"gzip\",\n",
    "            \"Authorization\": f\"Bearer {admin_token}\",\n",
    "        },\n",
    "        data=gzip.compress(json.dumps(indicator).encode(\"utf-8\")),\n",
    "        timeout=3600,\n",
    "    )\n",
    "    log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137b9c8-aece-417b-bdab-fd7ea33cafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/spatial_resolutions\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "spatial_resolutions = json.loads(response.content)[\"spatial_resolutions\"]\n",
    "srid_lookup = {f\"adm{sr['index']}\": sr[\"srid\"] for sr in spatial_resolutions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b5ae3-919a-463e-a79d-f457add0b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = httpx.get(\n",
    "    url=f\"{BASE_URL}/temporal_resolutions\", headers={\"Authorization\": f\"Bearer {admin_token}\"}\n",
    ")\n",
    "log(response)\n",
    "temporal_resolutions = json.loads(response.content)[\"temporal_resolutions\"]\n",
    "trid_lookup = {tr[\"relativedelta_unit\"]: tr[\"trid\"] for tr in temporal_resolutions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e16b77-166c-420d-b5db-227a76721acb",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Each file is a csv containing indicators for multiple dates for one category, at one spatial resolution and one temporal resolution. At present, there are not multiple resolutions per indicator, so we're assuming the files are just named for the category, e.g. `residents.csv`.\n",
    "\n",
    "We're going to rename the date and spatial columns, then create dataset files for them, which are a json representation structured:\n",
    "\n",
    "```\n",
    "{\n",
    "\"metadata\": {\n",
    "    \"revision\": <version>,\n",
    "    # adding a date here which will be overwritten later when it is actually added to the db\n",
    "    # this is to avoid a fastapi.exceptions.RequestValidationError for checking the length of a \"None\" type\n",
    "        \"date_added\": <datetime_now>,\n",
    "        \"category_id\": category_id,\n",
    "        \"indicator_id\": indicator_id,\n",
    "        \"srid\": <srid>,\n",
    "        \"trid\": <trid>,\n",
    "        \"dt\": <date>,\n",
    "    },\n",
    "    \"data_type\": <category>,\n",
    "    \"data_input\": [\n",
    "        {\n",
    "            \"spatial_unit_ids\": <list_of_ids>,\n",
    "            \"data\": <value>,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12a97d-bd7e-4897-b497-e9384ce56427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9568f-7066-4be0-992a-5fc6a2aeb1bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_data_files = {\n",
    "    \"residents\": {\n",
    "        \"category_id\": \"residents\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"months\"],\n",
    "    },\n",
    "    \"relocations\": {\n",
    "        \"category_id\": \"relocations\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"months\"],\n",
    "    },\n",
    "    \"presence\": {\n",
    "        \"category_id\": \"presence\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"days\"],\n",
    "    },\n",
    "    \"movements\": {\n",
    "        \"category_id\": \"movements\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"days\"],\n",
    "    },\n",
    "}\n",
    "synthetic_files = {\n",
    "    \"residents_admin3_monthly_small\": {\n",
    "        \"category_id\": \"residents\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"months\"],\n",
    "    },\n",
    "    \"relocations_admin3_monthly_small\": {\n",
    "        \"category_id\": \"relocations\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"months\"],\n",
    "    },\n",
    "    \"presence_admin3_daily_small\": {\n",
    "        \"category_id\": \"presence\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"days\"],\n",
    "    },\n",
    "    \"movements_admin3_daily_small\": {\n",
    "        \"category_id\": \"movements\",\n",
    "        \"srid\": srid_lookup[\"adm3\"],\n",
    "        \"trid\": trid_lookup[\"days\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "data_version = \"v1.0.2\"\n",
    "files = synthetic_files if SYNTHETIC else real_data_files\n",
    "\n",
    "parent_dir = f\"../impl/resources\"\n",
    "data_dir = f\"{parent_dir}/data/synthetic\" if SYNTHETIC else f\"{parent_dir}/data\"\n",
    "\n",
    "# Discard inf as well as na\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "\n",
    "\n",
    "def to_su_list(val):\n",
    "    if isinstance(val, tuple):\n",
    "        return [*val]\n",
    "    else:\n",
    "        return [val]\n",
    "\n",
    "\n",
    "for file_name, meta in files.items():\n",
    "    file_path = f\"{data_dir}/{file_name}.csv\"\n",
    "    preprocessed_path = f\"/tmp/{file_name}_preprocessed.csv\"\n",
    "    df = pd.read_csv(file_path)  # First column is the date\n",
    "    if file_name in [\"residents\", \"presence\"]:\n",
    "        # min columns: date, spatial unit, one data column\n",
    "        df = df.rename(columns={\"pcod\": \"spatial_unit\"})\n",
    "        df[\"date\"] = pd.to_datetime(df.date)\n",
    "        df = df.set_index([\"date\", \"spatial_unit\"])\n",
    "    elif file_name in [\"relocations\", \"movements\"]:\n",
    "        # min columns: date, 2 spatial units, one data column\n",
    "        df = df.rename(columns={\"month\": \"date\", \"pcod_from\": \"origin\", \"pcod_to\": \"destination\"})\n",
    "        df[\"date\"] = pd.to_datetime(df.date)\n",
    "        df = df.set_index([\"date\", \"origin\", \"destination\"])\n",
    "    df = df.sort_index()\n",
    "    for column in df.columns:\n",
    "        indicator_df = df[[column]].dropna()\n",
    "        dates = indicator_df.index.levels[0]\n",
    "        for dt in dates:\n",
    "            fname = f\"./tmp/{meta['category_id']}_{column}_{meta['srid']}_{meta['trid']}_{dt.strftime('%Y-%m-%dT%H:%M:%S')}_{data_version}.json\"\n",
    "            if pathlib.Path(fname).exists():\n",
    "                continue\n",
    "            try:\n",
    "                dataset = {\n",
    "                    \"metadata\": {\n",
    "                        \"revision\": data_version,\n",
    "                        # adding a date here which will be overwritten later when it is actually added to the db\n",
    "                        # this is to avoid a fastapi.exceptions.RequestValidationError for checking the length of a \"None\" type\n",
    "                        \"date_added\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                        \"category_id\": meta[\"category_id\"],\n",
    "                        \"indicator_id\": f\"{meta['category_id']}.{column}\",\n",
    "                        \"srid\": meta[\"srid\"],\n",
    "                        \"trid\": meta[\"trid\"],\n",
    "                        \"dt\": dt.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                    },\n",
    "                    \"data_type\": category_type_lookup[meta[\"category_id\"]],\n",
    "                    \"data_input\": [\n",
    "                        {\n",
    "                            \"spatial_unit_ids\": to_su_list(rw[0]),\n",
    "                            \"data\": rw[1],\n",
    "                        }\n",
    "                        for rw in indicator_df.loc[dt].itertuples()\n",
    "                    ],\n",
    "                }\n",
    "                fname = f\"./tmp/{meta['category_id']}_{column}_{meta['srid']}_{meta['trid']}_{dt.strftime('%Y-%m-%dT%H:%M:%S')}_{data_version}.json\"\n",
    "                with open(fname, \"w\") as fout:\n",
    "                    json.dump(dataset, fout)\n",
    "                    print(f\"Wrote {fname}\")\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d0db6-9296-431b-a9aa-a22c6a1c360e",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Now we can load the data we want to ingest. We'll glob all the files for the data version, and post them up. (In fact, we're going to send a PATCH so we replace what's there.)\n",
    "\n",
    "\n",
    "It's still recommended to compress the request body using `gzip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22e42-3fcf-4738-8f17-3369a94536b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(pathlib.Path(\"/tmp\").glob(f\"*_{data_version}.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ecc8a-c468-4ea2-9ad1-8289513ae478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 20\n",
    "\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "async def post_async(ds, client):\n",
    "    return await client.patch(\n",
    "        url=f\"{BASE_URL}/data\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Content-Encoding\": \"gzip\",\n",
    "            \"Authorization\": f\"Bearer {admin_token}\",\n",
    "        },\n",
    "        data=gzip.compress(json.dumps(ds, default=str).encode(\"utf-8\")),\n",
    "        timeout=3600,\n",
    "    )\n",
    "\n",
    "\n",
    "def yield_files_for_version(data_version):\n",
    "    files = pathlib.Path(\"tmp\").glob(f\"*_{data_version}.json\")\n",
    "    for fname in files:\n",
    "        with open(fname) as fin:\n",
    "            try:\n",
    "                yield json.load(fin)\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "                print(fname)\n",
    "\n",
    "\n",
    "def count_files_for_version(data_version):\n",
    "    return len(list(pathlib.Path(\"tmp\").glob(f\"*_{data_version}.json\")))\n",
    "\n",
    "\n",
    "async def ingest_data_version(data_version, chunksize):\n",
    "    n_datasets = count_files_for_version(data_version)\n",
    "    print(f\"Starting ingestion of data for {count_files_for_version(data_version)} indicators...\")\n",
    "    num = 0\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        for chunk in chunked_iterable(yield_files_for_version(data_version), size=chunksize):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            responses = await asyncio.gather(*(post_async(ds, client) for ds in chunk))\n",
    "            for response in responses:\n",
    "                if response.status_code not in [201, 204]:\n",
    "                    print(\"\")\n",
    "                    log(response)\n",
    "                else:\n",
    "                    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57b0b5-95f0-4772-8d2f-2d5a02e27af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await ingest_data_version(data_version, CHUNK_SIZE)\n",
    "# loop = asyncio.get_event_loop()\n",
    "# task = loop.create_task(doit())\n",
    "# if not loop.is_running():\n",
    "#    loop.run_until_complete(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbcf12-d4b3-4328-ae27-39e1aa4c6392",
   "metadata": {},
   "source": [
    "Done! Provided you got all `201` or `204` responses (i.e. no errors), the data should now be in the database!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e909784-5a22-4105-93da-48644986ede8",
   "metadata": {},
   "source": [
    "## Data permissions & access\n",
    "\n",
    "The data is now in the database, but without access management, only administrators will be able to see the data by default.\n",
    "To enable access by users depending on their roles, we need to define what scopes give access to which part of the data.\n",
    "We use an \"allow-list\" style access management so we have to define each bit of data that will be accessible to users that aren't admins.\n",
    "We do that using JSON. Each key is the name of a scope as defined in Auth0 (see also the API spec) and each value is a set of queries (as per API spec) that define a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef4661-f5c1-485e-8600-59ec1a346bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_access = {\n",
    "    \"read:free_data\": {\n",
    "        \"start_date\": \"2020-01\",\n",
    "        \"duration\": 5,\n",
    "    },\n",
    "    \"read:premium_data\": {\n",
    "        \"start_date\": \"2020-01\",\n",
    "        \"duration\": 9999,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1fca3-a1aa-4b9a-9030-f6a33cc6ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trid_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ac8e8-0491-4b76-a54b-58cf8c03fccb",
   "metadata": {},
   "source": [
    "Now we need to get the metadata IDs of the specified data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85442e50-590c-4665-b45c-eb1579486aab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope_mappings = []\n",
    "for scope, query in data_access.items():\n",
    "    for indicator in indicators:\n",
    "        query_to_sub = dict(**query)\n",
    "        query_to_sub[\"mdids_only\"] = True\n",
    "        query_to_sub[\"category_id\"] = indicator[\"category_id\"]\n",
    "        query_to_sub[\"indicator_id\"] = indicator[\"indicator_id\"]\n",
    "        query_to_sub[\"srid\"] = files[indicator[\"category_id\"]][\"srid\"]\n",
    "        query_to_sub[\"trid\"] = files[indicator[\"category_id\"]][\"trid\"]\n",
    "        if files[indicator[\"category_id\"]][\"trid\"] == trid_lookup[\"days\"]:\n",
    "            query_to_sub[\"duration\"] = query[\"duration\"] * 28\n",
    "        response = httpx.post(\n",
    "            url=f\"{BASE_URL}/query\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Content-Encoding\": \"gzip\",\n",
    "                \"Authorization\": f\"Bearer {admin_token}\",\n",
    "            },\n",
    "            data=gzip.compress(json.dumps(query_to_sub).encode(\"utf-8\")),\n",
    "        )\n",
    "        scope_mappings += [(scope, mdid) for mdid in json.loads(response.content)[\"mdids\"]]\n",
    "print(scope_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af944e3-0e61-4181-8925-01d0a400c446",
   "metadata": {},
   "source": [
    "Next we can ingest the scope mappings using the `/scope_mapping` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509e64e-ff74-4860-93c8-19ef269312ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de822c81-fca9-4504-b8f1-1c91504855fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for scope, mdid in scope_mappings:\n",
    "    scope_mapping = {\"scope\": scope, \"mdid\": mdid}\n",
    "    response = httpx.post(\n",
    "        url=f\"{BASE_URL}/scope_mapping\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Content-Encoding\": \"gzip\",\n",
    "            \"Authorization\": f\"Bearer {admin_token}\",\n",
    "        },\n",
    "        data=gzip.compress(json.dumps(scope_mapping).encode(\"utf-8\")),\n",
    "    )\n",
    "    if response.status_code not in [201, 204, 303]:\n",
    "        log(response)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da760278-811b-4c72-93e0-8be6321f5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = httpx.post(\n",
    "    url=f\"{BASE_URL}/scope_mapping\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Content-Encoding\": \"gzip\",\n",
    "        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    },\n",
    "    data=gzip.compress(json.dumps(scope_mapping).encode(\"utf-8\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc7c67-1e45-4d4e-9e88-50ffcea62ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "async with httpx.AsyncClient() as client:\n",
    "    for chunk in chunked_iterable(scope_mappings, size=25):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        responses = await asyncio.gather(\n",
    "            *(\n",
    "                client.post(\n",
    "                    url=f\"{BASE_URL}/scope_mapping\",\n",
    "                    headers={\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "                    },\n",
    "                    data=json.dumps({\"scope\": scope, \"mdid\": mdid}).encode(\"utf-8\"),\n",
    "                )\n",
    "                for scope, mdid in chunk\n",
    "            )\n",
    "        )\n",
    "        for response in responses:\n",
    "            if response.status_code not in [201, 204]:\n",
    "                pass\n",
    "            else:\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360f81a-a785-4e69-8a06-203d712fb526",
   "metadata": {},
   "source": [
    "If not status codes other than `204` come back, then the ingestion of scope mappings worked and the data is now tagged.\n",
    "\n",
    "Note that to delete data, you need to use the `httpx.request()` method as in the example below.\n",
    "The reason is that the `httpx.delete()` method does not support a body although the spec does not explicitly forbid it.\n",
    "\n",
    "```python\n",
    "response = httpx.request(\n",
    "    url=f\"{BASE_URL}/scope_mapping\",\n",
    "    method=\"DELETE\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Content-Encoding\": \"gzip\",\n",
    "        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    },\n",
    "    data=gzip.compress(json.dumps(scope_mapping).encode(\"utf-8\")),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f395782-ce21-4d6d-bbfd-bc5d40842dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indicator in indicators:\n",
    "    for d\n",
    "        query_to_sub = {\n",
    "            \"start_date\": \"2020-01\",\n",
    "            \"duration\": 1,\n",
    "        }\n",
    "        query_to_sub[\"mdids_only\"] = True\n",
    "        query_to_sub['category_id'] = indicator['category_id']\n",
    "        query_to_sub['indicator_id'] = indicator['indicator_id']\n",
    "        query_to_sub['srid'] = files[indicator['category_id']]['srid']\n",
    "        query_to_sub['trid'] = files[indicator['category_id']]['trid']\n",
    "        if files[indicator['category_id']]['trid'] == trid_lookup['days']:\n",
    "            query_to_sub['duration'] = query['duration']*28\n",
    "        response = httpx.post(\n",
    "            url=f\"{BASE_URL}/query\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Content-Encoding\": \"gzip\",\n",
    "                \"Authorization\": f\"Bearer {admin_token}\",\n",
    "            },\n",
    "            data=gzip.compress(json.dumps(query_to_sub).encode(\"utf-8\")),\n",
    "        )\n",
    "        scope_mappings += [(scope, mdid) for mdid in json.loads(response.content)[\"mdids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15558142-8be9-4cbe-96df-6a3e63e4cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out non 1.0.2 data\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    for chunk in chunked_iterable(scope_mappings, size=30):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        responses = await asyncio.gather(\n",
    "            *(\n",
    "                client.post(\n",
    "                    url=f\"{BASE_URL}/scope_mapping\",\n",
    "                    headers={\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": f\"Bearer {admin_token}\",\n",
    "                    },\n",
    "                    data=json.dumps({\"scope\": scope, \"mdid\": mdid}).encode(\"utf-8\"),\n",
    "                )\n",
    "                for scope, mdid in chunk\n",
    "            )\n",
    "        )\n",
    "        for response in responses:\n",
    "            if response.status_code not in [201, 204]:\n",
    "                print(\"\")\n",
    "                log(response)\n",
    "            else:\n",
    "                num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c180f8ebda99849f742c1970511da7e002d61838aeb8176b3be05803a158b625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
